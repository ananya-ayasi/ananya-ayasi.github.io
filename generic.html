<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Ananya Ayasi</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Research Experience</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li class="active"><a href="generic.html">Research Experience</a></li>
							<li><a href="elements.html">Volunteering</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://github.com/ananya-ayasi" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="https://www.linkedin.com/in/ananya-ayasi-0b654a166/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<span class="date">Academia</span>
								</header>
								<div id="work-section">
										<p>
											<b>wav2gloss: Speech to Glossed Text of Low Resource Languages </b> 
										</p>
										<ul>
											<li>
												Guided by <a href="https://www.cs.cmu.edu/~lsl/">Prof. Lori</a>, <a href="https://sites.google.com/view/shinjiwatanabe">Prof. Shinji Watanabe</a> and <a href="https://www.cs.cmu.edu/~dmortens/">Prof. David Mortensen</a>
											</li>
											<li> Automatically generate Interlinear Glossed Text of low-resource languages from audio files.</li>
											<li>Focusing on the speech enhancement of audio files using ESPNet2 to improve the ASR performance.</li>
											
										</ul>
										<p>
											<b>Multilingual TTS Accent Impressions for Accented ASR </b> 
										</p>
										<ul>
											<li>
												Guided by <a href="https://sites.google.com/view/shinjiwatanabe">Prof. Shinji Watanabe</a> and <a href="https://www.cs.cmu.edu/~dmortens/">Prof. David Mortensen</a>
											</li>
											<li> Fine-tuning of English ASR systems for L2-English speakers.</li>
											<li>Paper accepted to Text, Speech and Dialogue Conference (Sept 4-7, 2023)</li>
											
										</ul>
										<p>
											<b>Speaker Diariazation using Deep Learning Techniques</b> 
										</p>
										<ul>
											<li>
												Guided by <a href="http://www.rajeevrajan.in/">Dr. Rajeev Rajan</a>
											</li>
											<li>Implemented Bi-LSTM and Bi-GRU models with self-attention as baseline model and obtained an error rate of 25.7%.</li>
											<li>Experimented with various deep learning techniques involving transformers to diarize a given audio with data
												obtained from AMI Speech Corpus</li>
											
											<li>Paper accepted to ICNGIS 2022</li>
											
										</ul>
										<p>
											<b>Octoechos classification in liturgical music using Deep Learning frameworks</b> 
										</p>
										<ul>
											<li>
												Guided by <a href="http://www.rajeevrajan.in/">Dr. Rajeev Rajan</a>
											</li>
											<li>
												This automatic classification scheme is addressed using MTF-GRU, MTF-LSTM, MTF-SBU-LSTM, MTF-SB-GRU and MTF-SBU-GRU. The performance of these methods is compared using their accuracy (precision, recall and f1-score).</a>
											</li>
											<li>
												The proposed experiment demonstrates the potential of SBU-LSTM in learning information through MTF.    
											</li>
											<li>
												Paper accepted to INTERSPEECH 2022.   
											</li>          
										</ul>
										<p>
											<b>Understanding the scope of quantum technology to gauge its social impact </b> - <em>Independent and Self-started</em> 
										</p>
										<ul>
											<li>
												Presented a research poster for “Careers in Quantum” conducted by QET Labs and University of Bristol.
											</li>
											<li>
												Gave a talk on  <a href="https://www.womentech.net/speaker/Ananya/Ayasi/58004">Quantum Technology and its Impact on Society</a> for WomenTech Global Conference 2021.
											</li>
											<li>
												Gave a talk on <a href="https://www.linkedin.com/posts/ananya-ayasi-0b654a166_vocalize-is-an-initiative-by-women-in-engineering-activity-6718212110821703680-nvYW">Basics of Quantum Computing”</a> as the first speaker of “Vocalize”, conducted by IEEE Travancore Hub on 4th October, 2020. 
											</li>
											
											
										</ul>
									<br>
					
									
								</div>
							</section>

					</div>

				
				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Ananya</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
